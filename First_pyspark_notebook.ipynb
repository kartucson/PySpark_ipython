{
 "metadata": {
  "name": "",
  "signature": "sha256:644f03a7e1775119ccdf6e26efadf9b595cc4c09eb1622542e28bbbb589ab9d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "from pyspark import SparkContext \n",
      "from pyspark import SparkConf\n",
      "# Sys.setenv('SPARKR_SUBMIT_ARGS'='\"--packages\" \"com.databricks:spark-csv_2.10:1.0.3\" \"sparkr-shell\"') for R\n",
      "### MANNN restart spart using ipython notebook --profile=pyspark --packages com.databricks:spark-csv_2.10:1.0.3  \n",
      "os.environ['SPARK_HOME']=\"G:/Spark/spark-1.5.1-bin-hadoop2.6\"\n",
      "\n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/bin\") \n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python\") \n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/pyspark/\") \n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql\")\n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/pyspark/mllib\")\n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/lib\") \n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip\")\n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\") \n",
      "sys.path.append(\"G:/Spark/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip\")\n",
      "\n",
      "##sc.stop() # IF you wish to stop the context\n",
      "sc = SparkContext(\"local\", \"Simple App\")\n",
      "\n",
      "sqlContext = SQLContext(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'SQLContext' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-a75ced57d883>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Simple App\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'SQLContext' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logFile = \"README.md\"  # Should be some file on your system\n",
      "sc = SparkContext(\"local\", \"Simple App\")\n",
      "logData = sc.textFile(logFile).cache()\n",
      "\n",
      "numAs = logData.filter(lambda s: 'a' in s).count()\n",
      "numBs = logData.filter(lambda s: 'b' in s).count()\n",
      "\n",
      "print \"Lines with a: %i, lines with b: %i\" % (numAs, numBs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'SparkContext' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-793ae9204c78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"README.md\"\u001b[0m  \u001b[1;31m# Should be some file on your system\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Simple App\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlogData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnumAs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'a'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'SparkContext' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql import SQLContext\n",
      "from pyspark.sql.types import *\n",
      "#import pyspark_csv as pycsv  See  https://github.com/seahboonsiew/pyspark-csv\n",
      "sqlContext = SQLContext(sc)\n",
      "#userhash = sc.textFile(\"DTM_tw_retw1.csv\")\n",
      "gsa = sc.textFile(\"Third_dataset.csv\")\n",
      "#gsa.count()\n",
      "#import sparklingpandas\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gsa.take(5)\n",
      "header = gsa.first()\n",
      "schemaString = header.replace('\"','')\n",
      "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(',')]\n",
      "#print userhash.take(5)\n",
      "#header = gsa.first()\n",
      "#print header\n",
      "#sparkDF = sqlContext.read.format('com.databricks.spark.csv').options(header='true',inferschema='true').load('Third_dataset.csv')\n",
      "#print sparkDF.count()\n",
      "#print sparkDF.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'\"\",\"SDNN\",\"RMSSD\",\"ID\",\"Timestamp\",\"Activity\",\"ToD\",\"DoW\",\"Gender\",\"Age\",\"BMI\",\"Pressure\",\"Relative_humidity\",\"Temperature\",\"CO2\",\"Sound\",\"Wall_co\",\"Wall_co2\",\"Wall_humidity\",\"Wall_light\",\"Wall_pm_1micron\",\"Wall_pm_2.5micron\",\"Wall_pressure\",\"Wall_sound\",\"Wall_temperature\",\"Wall_VOC\"', u'\"1\",69.28408291,24.05146765,101,\"5/12/2015 9:24\",0.302218166,\"Morning\",\"Tuesday\",\"Female\",22,20.59570313,NA,NA,NA,NA,NA,0.301303995,461.4026896,55.85639127,0.035337586,0,0,1013.777312,42.4648291,20.52999932,NA', u'\"2\",72.30945548,22.10825254,101,\"5/12/2015 9:25\",0.211393129,\"Morning\",\"Tuesday\",\"Female\",22,20.59570313,NA,NA,NA,NA,NA,0.297586977,459.019722,55.86027463,0.035507627,0,0,1013.805668,42.46670136,20.53833268,NA', u'\"3\",70.99656111,23.75568419,101,\"5/12/2015 9:26\",0.060598673,\"Morning\",\"Tuesday\",\"Female\",22,20.59570313,NA,NA,NA,NA,NA,0.295172815,458.2117615,55.86023649,0.034926,0,0,1013.809441,42.48238293,20.53666598,NA', u'\"4\",68.5289037,25.11272332,101,\"5/12/2015 9:27\",0.084183105,\"Morning\",\"Tuesday\",\"Female\",22,20.59570313,NA,NA,NA,NA,NA,0.304844293,459.0082016,55.86796443,0.0351125,0,0,1013.870779,42.48220705,20.53166587,NA']\n"
       ]
      }
     ],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}